<!DOCTYPE html>
<html>
<head>
    <title>AI Debug Scanner</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="edge-impulse-standalone.js"></script>
    <script src="run-impulse.js"></script>
    <style>
        body { font-family: sans-serif; background: #121212; color: white; text-align: center; margin: 0; }
        #camera { width: 100%; max-width: 500px; background: #000; }
        #status { background: #333; padding: 15px; font-weight: bold; color: #fbff00; }
        #viewfinder { border: 2px solid #4CAF50; position: fixed; top: 10px; right: 10px; width: 100px; height: 100px; object-fit: cover; }
    </style>
</head>
<body>
    <video id="camera" autoplay playsinline></video>
    <canvas id="viewfinder" width="320" height="320"></canvas>
    
    <div id="status">Starting AI...</div>
    <div id="results" style="padding: 20px; font-size: 1.5em;"></div>

    <script>
        async function startScanner() {
            const video = document.getElementById('camera');
            const status = document.getElementById('status');
            const resultsDiv = document.getElementById('results');
            const canvas = document.getElementById('viewfinder');
            const ctx = canvas.getContext('2d');

            try {
                const classifier = new EdgeImpulseClassifier();
                await classifier.init(); [cite: 10]
                
                const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" } }); [cite: 10]
                video.srcObject = stream;
                status.innerText = "AI ACTIVE - SCANNING";

                async function loop() {
                    // 1. Capture and resize for the AI
                    ctx.drawImage(video, 0, 0, 320, 320);
                    const imgData = ctx.getImageData(0, 0, 320, 320);
                    
                    // 2. Convert pixels to Float32 format (required by run-impulse.js)
                    const floatData = [];
                    for (let i = 0; i < imgData.data.length; i += 4) {
                        floatData.push(imgData.data[i]);     // R
                        floatData.push(imgData.data[i + 1]); // G
                        floatData.push(imgData.data[i + 2]); // B
                    }

                    // 3. Classify
                    const result = await classifier.classify(floatData); [cite: 14]
                    
                    if (result.results && result.results.length > 0) {
                        // Filter out "Background" and show the top match
                        let match = result.results
                            .filter(r => r.label !== "Background")
                            .reduce((prev, current) => (prev.value > current.value) ? prev : current);

                        if (match && match.value > 0.10) { // Very low 10% threshold for testing
                            status.innerText = `DETECTED: ${match.label.toUpperCase()}`;
                            resultsDiv.innerHTML = `Identified: <b>${match.label}</b><br>Confidence: ${(match.value * 100).toFixed(1)}%`;
                        }
                    }
                    requestAnimationFrame(loop);
                }
                video.onplay = loop;

            } catch (e) {
                status.innerText = "Error: " + e.message;
            }
        }
        window.onload = startScanner;
    </script>
</body>
</html>
